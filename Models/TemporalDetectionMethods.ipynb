{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD-LulHCt3w3",
        "outputId": "811fb879-61f2-4574-ddc1-abab1b3ea30a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to C:\\Users\\salma/.cache\\torch\\hub\\main.zip\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fvcore'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#To install certain libraries that will be used in pytorchvideo\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacebookresearch/pytorchvideo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mslowfast_r50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
            "File \u001b[1;32mc:\\Users\\salma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:566\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    563\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    564\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[1;32m--> 566\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Users\\salma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:592\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[0;32m    591\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m--> 592\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m    595\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\salma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:106\u001b[0m, in \u001b[0;36m_import_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    104\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\hubconf.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401, E402\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     c2d_r50,\n\u001b[0;32m      6\u001b[0m     csn_r101,\n\u001b[0;32m      7\u001b[0m     efficient_x3d_s,\n\u001b[0;32m      8\u001b[0m     efficient_x3d_xs,\n\u001b[0;32m      9\u001b[0m     i3d_r50,\n\u001b[0;32m     10\u001b[0m     mvit_base_16,\n\u001b[0;32m     11\u001b[0m     mvit_base_16x4,\n\u001b[0;32m     12\u001b[0m     mvit_base_32x3,\n\u001b[0;32m     13\u001b[0m     r2plus1d_r50,\n\u001b[0;32m     14\u001b[0m     slow_r50,\n\u001b[0;32m     15\u001b[0m     slow_r50_detection,\n\u001b[0;32m     16\u001b[0m     slowfast_16x8_r101_50_50,\n\u001b[0;32m     17\u001b[0m     slowfast_r101,\n\u001b[0;32m     18\u001b[0m     slowfast_r50,\n\u001b[0;32m     19\u001b[0m     slowfast_r50_detection,\n\u001b[0;32m     20\u001b[0m     x3d_l,\n\u001b[0;32m     21\u001b[0m     x3d_m,\n\u001b[0;32m     22\u001b[0m     x3d_s,\n\u001b[0;32m     23\u001b[0m     x3d_xs,\n\u001b[0;32m     24\u001b[0m )\n",
            "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\pytorchvideo\\models\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_csn\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_res_basic_head, ResNetBasicHead\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasked_multistream\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     LearnMaskedDefault,\n\u001b[0;32m      7\u001b[0m     LSTM,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     TransposeTransformerEncoder,\n\u001b[0;32m     13\u001b[0m )\n",
            "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\pytorchvideo\\models\\csn.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_res_basic_head\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_bottleneck_block, create_res_stage, Net\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_res_basic_stem\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_csn\u001b[39m(\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Input clip configs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     head_output_with_global_average: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n",
            "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\pytorchvideo\\models\\resnet.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_attributes\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_res_basic_head, create_res_roi_pooling_head\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectionBBoxNetwork, Net\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     create_acoustic_res_basic_stem,\n\u001b[0;32m     13\u001b[0m     create_res_basic_stem,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_bottleneck_block\u001b[39m(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Convolution configs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     activation: Callable \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU,\n\u001b[0;32m     40\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n",
            "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\pytorchvideo\\models\\net.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_attributes\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweight_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_net_weights\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    Build a general Net models with a list of blocks for video recognition.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    The ResNet builder can be found in `create_resnet`.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\pytorchvideo\\models\\weight_init.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfvcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweight_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m c2_msra_fill, c2_xavier_fill\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpatioTemporalClsPositionalEncoding\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_resnet_weights\u001b[39m(model: nn\u001b[38;5;241m.\u001b[39mModule, fc_init_std: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fvcore'"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "import torch\n",
        "import os\n",
        "#To install certain libraries that will be used in pytorchvideo\n",
        "MODEL = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
        "import torchvision.io as io\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")\n",
        "from pytorchvideo.models import slowfast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Au00-kk5GUoE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ModifiedSlowFast(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(ModifiedSlowFast, self).__init__()\n",
        "\n",
        "        # Copy all layers except the head from the original model\n",
        "        self.features = nn.Sequential(*list(original_model.blocks.children())[:-1])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the modified SlowFast model\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UiHfLF5ouHMn"
      },
      "outputs": [],
      "source": [
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 1\n",
        "frames_per_second = 30\n",
        "slowfast_alpha = 4\n",
        "num_clips = 10\n",
        "num_crops = 3\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H13f_XSGuIBF"
      },
      "outputs": [],
      "source": [
        "class TemporalDetection:\n",
        "  def __init__(self, video_path, output_dir):\n",
        "    self.model_feature_extraction_slowfast = torch.load('slowfast_1sec_extractore.h5')\n",
        "    self.video_path = video_path\n",
        "    self.MLP_Model = tf.keras.models.load_model('best_model_standarized.h5')\n",
        "    self.video = EncodedVideo.from_path( video_path )\n",
        "    self.output_dir = output_dir\n",
        "  def extract_features(self, ):\n",
        "    # model_feature_extraction_slowfast = torch.load('/content/drive/MyDrive/Tics Dataset/SlowfastTrial/models/slowfast_1sec_extractore.h5')\n",
        "    # video = EncodedVideo.from_path( video_path )\n",
        "    num_of_clips= int(self.video.duration) // int(1.0666666666666667)\n",
        "    print(\"Video clips: \", num_of_clips)\n",
        "    all_features = np.zeros((num_of_clips, 2304))\n",
        "    for clip_index, start_sec in enumerate(range(0, int(self.video.duration), int(clip_duration))):\n",
        "          end_sec = min(start_sec + clip_duration, self.video.duration)\n",
        "\n",
        "          # Load the desired clip (replace with your actual clip loading logic)\n",
        "          video_data = self.video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "          # Apply the transform\n",
        "          video_data = transform(video_data)\n",
        "\n",
        "          # Move the inputs to the desired device\n",
        "          inputs = video_data[\"video\"]\n",
        "          inputs = [i.to(device)[None, ...] for i in inputs]\n",
        "\n",
        "          with torch.no_grad():\n",
        "              features = self.model_feature_extraction_slowfast(inputs)\n",
        "          # Convert the features to a NumPy array\n",
        "          features = features.cpu().numpy()\n",
        "          features = features.reshape(features.shape[0], -1)\n",
        "          if (start_sec <2):\n",
        "                print(features.shape)\n",
        "                all_features[clip_index] = features\n",
        "    return all_features\n",
        "  def make_predictions(self,):\n",
        "    scaler = StandardScaler()\n",
        "    features = self.extract_features()\n",
        "    print(len(features))\n",
        "    features = scaler.fit_transform(features)\n",
        "    predictions = self.MLP_Model.predict(features)\n",
        "    Binary_predictions = (predictions> 0.5).astype(\"int32\")\n",
        "    return Binary_predictions\n",
        "  def concatenated_clips(self, labels):\n",
        "    clips = []\n",
        "    temp_clips = []\n",
        "    for count, prediction in enumerate(labels):\n",
        "      if prediction == 1 or prediction == 1.0:\n",
        "        # print('entered')\n",
        "        temp_clips.append(count)\n",
        "      else:\n",
        "        clips.append(temp_clips)\n",
        "        temp_clips = []\n",
        "    if prediction == 1:\n",
        "      clips.append(temp_clips)\n",
        "      temp_clips = []\n",
        "    print(temp_clips)\n",
        "    clips = [sub_clip for sub_clip in clips if sub_clip]\n",
        "    clips_by_one = clips\n",
        "    tics_count =  sum(len(sub_array) for sub_array in clips)\n",
        "    clips = [[sub_clip[0], sub_clip[-1]] for sub_clip in clips if sub_clip]\n",
        "    print(clips)\n",
        "    return clips, tics_count, clips_by_one\n",
        "  def save_video(self, video_tensor, output_path, fps=30):\n",
        "      video_tensor = video_tensor.permute(1, 2, 3, 0)\n",
        "      io.write_video(output_path, video_tensor, fps)\n",
        "  def Temporal_Detection(self, ):\n",
        "      preds = self.make_predictions()\n",
        "      preds = np.array(preds).reshape(-1)\n",
        "      clips_in_ranges, tics_count, clips = self.concatenated_clips(preds)\n",
        "      for i, (start, end) in enumerate(clips):\n",
        "          video_tensor = self.video.get_clip(start, end)\n",
        "          output_file = os.path.join(self.output_dir, f'clip_{i+1}.mp4')\n",
        "          self.save_video(video_tensor['video'], output_file)\n",
        "      return {'clips_in_ranges': clips_in_ranges, \"output_path\": self.output_dir, 'tics_count': tics_count, 'clips': clips, 'predictions':preds}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xxej6Gwr3AlW"
      },
      "outputs": [],
      "source": [
        "detect_tics = TemporalDetection('../downloaded_videos/subject1_video9.mp4',\n",
        "                                '../uploads')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "Osy57SEyeuK1",
        "outputId": "510e5e0a-0bda-4bdc-adab-0b6a8029b1fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video clips:  402\n",
            "(1, 2304)\n",
            "(1, 2304)\n",
            "402\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "[]\n",
            "[[2, 401]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m temporals \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_tics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTemporal_Detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn [13], line 71\u001b[0m, in \u001b[0;36mTemporalDetection.Temporal_Detection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(preds)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     70\u001b[0m clips_in_ranges, tics_count, clips \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcatenated_clips(preds)\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (start, end) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(clips):\n\u001b[0;32m     72\u001b[0m     video_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mget_clip(start, end)\n\u001b[0;32m     73\u001b[0m     output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "temporals = detect_tics.Temporal_Detection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMN2vdeXgmxk"
      },
      "outputs": [],
      "source": [
        "temporals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRqJo2shLuuj"
      },
      "outputs": [],
      "source": [
        "def concateenated_clips(labels):\n",
        "  clips = []\n",
        "  temp_clips = []\n",
        "  for count, prediction in enumerate(labels):\n",
        "    if prediction == 1:\n",
        "      temp_clips.append(count)\n",
        "    elif prediction == 0:\n",
        "      clips.append(temp_clips)\n",
        "      temp_clips = []\n",
        "  clips = [sub_clip for sub_clip in clips if sub_clip]\n",
        "  clips = [[sub_clip[0], sub_clip[-1]] for sub_clip in clips if sub_clip]\n",
        "  return clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPBUi1hkEGhT",
        "outputId": "bdc6043d-696e-4a6b-c1c6-1ef4f17c29f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video clips:  402\n",
            "(1, 2304)\n",
            "(1, 2304)\n",
            "13/13 [==============================] - 0s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "video_path = '/content/drive/MyDrive/Tics Dataset/videos/extracted videos/subject1_video9.mp4'\n",
        "preds = Temporal_Detection(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCIYXFWVLxIG"
      },
      "outputs": [],
      "source": [
        "clips = concateenated_clips(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhG8JakZW2xz"
      },
      "source": [
        "### Tell here the Temporal Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wu7ZqbEV_11"
      },
      "outputs": [],
      "source": [
        "true = pd.read_csv('/content/drive/MyDrive/Tics Dataset/SlowfastTrial/1sec_Sampling/AllCombined/1_sec_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w1B5oy8WGK3"
      },
      "outputs": [],
      "source": [
        "y_true = true[true['Video_number']==9]['labels']\n",
        "y_true = y_true.map({'normal':0, 'tic':1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdvCWVykVwon",
        "outputId": "4bd544f0-0923-4a91-9386-40064b57ee98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.17164179104477612"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(preds.reshape(-1), y_true.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT3WRW_oINGM",
        "outputId": "acc5fdab-f184-45fa-81ec-23599d633543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video clips:  402\n",
            "(1, 2304)\n"
          ]
        }
      ],
      "source": [
        "features = extract_features(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDTWoJpiIS7D",
        "outputId": "9ffbb2b5-1cf3-472f-cf75-87d1917bb2ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1, 2304, 1, 1, 1)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaOy9CoHFkj2",
        "outputId": "d65d8550-3df3-44a7-fc85-eddf944312ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7a02415555d0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import glob\n",
        "# glob.glob('/content/drive/MyDrive/Tics Dataset/SlowfastTrial/models/1 sec classification (tics temporal detection)/goodmodel.h5')\n",
        "# tf.keras.models.load_model('/content/drive/MyDrive/Tics Dataset/SlowfastTrial/models/1 sec classification (tics temporal detection)/goodmodel.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eFJ4I1bGtLi",
        "outputId": "36f444cf-d8e1-4a8d-db56-22582c124fb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2304)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# np.load('/content/drive/MyDrive/Tics Dataset/SlowfastTraial/1sec_Sampling/subject2_features/subject2_video0/patient3_video0_clip_0.npy').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxSA91A0HNZ3",
        "outputId": "bb0f384c-ad01-4326-c6da-06c80e362e7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 1, 0])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoLVUkAbHTLZ"
      },
      "outputs": [],
      "source": [
        "  def concateenated_clips( labels):\n",
        "    clips = []\n",
        "    temp_clips = []\n",
        "    for count, prediction in enumerate(labels):\n",
        "      if prediction == 1:\n",
        "        temp_clips.append(count)\n",
        "      elif prediction == 0:\n",
        "        clips.append(temp_clips)\n",
        "        temp_clips = []\n",
        "    clips = [sub_clip for sub_clip in clips if sub_clip]\n",
        "    clips_by_one = clips\n",
        "    tics_count =  sum(len(sub_array) for sub_array in clips)\n",
        "    clips = [[sub_clip[0], sub_clip[-1]] for sub_clip in clips if sub_clip]\n",
        "    return clips, tics_count, clips_by_one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPlx5vbjJHVQ",
        "outputId": "fead2a9f-ee20-4d76-a6f7-a40721a8eb02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "clips, count, values = concateenated_clips([0,1,1,1,1,1,1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5PuH_8ELJFv",
        "outputId": "7cdbd245-9bde-4bcd-e036-9a6acf837504"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([[1, 7]], 7, [[1, 2, 3, 4, 5, 6, 7]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clips, count, values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gs9asFDlwuP",
        "outputId": "c45a7f00-f425-477e-d39d-719b3b19835b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n"
          ]
        }
      ],
      "source": [
        "for i, val in enumerate(temporals['predictions'].tolist()):\n",
        "  # print(val, val == 1)\n",
        "  if val == 1:\n",
        "    print(i)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
