{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD-LulHCt3w3",
        "outputId": "811fb879-61f2-4574-ddc1-abab1b3ea30a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to C:\\Users\\hp/.cache\\torch\\hub\\main.zip\n",
            "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth\" to C:\\Users\\hp/.cache\\torch\\hub\\checkpoints\\SLOWFAST_8x8_R50.pyth\n",
            "100%|██████████| 264M/264M [01:17<00:00, 3.55MB/s] \n",
            "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "import torch\n",
        "import os\n",
        "#To install certain libraries that will be used in pytorchvideo\n",
        "MODEL = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
        "import torchvision.io as io\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")\n",
        "from pytorchvideo.models import slowfast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Au00-kk5GUoE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ModifiedSlowFast(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(ModifiedSlowFast, self).__init__()\n",
        "\n",
        "        # Copy all layers except the head from the original model\n",
        "        self.features = nn.Sequential(*list(original_model.blocks.children())[:-1])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the modified SlowFast model\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UiHfLF5ouHMn"
      },
      "outputs": [],
      "source": [
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 1\n",
        "frames_per_second = 30\n",
        "slowfast_alpha = 4\n",
        "num_clips = 10\n",
        "num_crops = 3\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H13f_XSGuIBF"
      },
      "outputs": [],
      "source": [
        "class TemporalDetection:\n",
        "  def __init__(self, video_path, output_dir):\n",
        "    self.model_feature_extraction_slowfast = torch.load('slowfast_1sec_extractore.h5')\n",
        "    self.video_path = video_path\n",
        "    self.MLP_Model = tf.keras.models.load_model('best_model_standarized.h5')\n",
        "    self.video = EncodedVideo.from_path( video_path )\n",
        "    self.output_dir = output_dir\n",
        "  def extract_features(self, ):\n",
        "    # model_feature_extraction_slowfast = torch.load('/content/drive/MyDrive/Tics Dataset/SlowfastTrial/models/slowfast_1sec_extractore.h5')\n",
        "    # video = EncodedVideo.from_path( video_path )\n",
        "    num_of_clips= int(self.video.duration) // int(1.0666666666666667)\n",
        "    print(\"Video clips: \", num_of_clips)\n",
        "    all_features = np.zeros((num_of_clips, 2304))\n",
        "    for clip_index, start_sec in enumerate(range(0, int(self.video.duration), int(clip_duration))):\n",
        "          end_sec = min(start_sec + clip_duration, self.video.duration)\n",
        "\n",
        "          # Load the desired clip (replace with your actual clip loading logic)\n",
        "          video_data = self.video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "          # Apply the transform\n",
        "          video_data = transform(video_data)\n",
        "\n",
        "          # Move the inputs to the desired device\n",
        "          inputs = video_data[\"video\"]\n",
        "          inputs = [i.to(device)[None, ...] for i in inputs]\n",
        "\n",
        "          with torch.no_grad():\n",
        "              features = self.model_feature_extraction_slowfast(inputs)\n",
        "          # Convert the features to a NumPy array\n",
        "          features = features.cpu().numpy()\n",
        "          features = features.reshape(features.shape[0], -1)\n",
        "          if (start_sec <2):\n",
        "                print(features.shape)\n",
        "                all_features[clip_index] = features\n",
        "    return all_features\n",
        "  def make_predictions(self,):\n",
        "    scaler = StandardScaler()\n",
        "    features = self.extract_features()\n",
        "    print(len(features))\n",
        "    features = scaler.fit_transform(features)\n",
        "    predictions = self.MLP_Model.predict(features)\n",
        "    Binary_predictions = (predictions> 0.5).astype(\"int32\")\n",
        "    return Binary_predictions\n",
        "  def concatenated_clips(self, labels):\n",
        "    clips = []\n",
        "    temp_clips = []\n",
        "    for count, prediction in enumerate(labels):\n",
        "      if prediction == 1 or prediction == 1.0:\n",
        "        # print('entered')\n",
        "        temp_clips.append(count)\n",
        "      else:\n",
        "        clips.append(temp_clips)\n",
        "        temp_clips = []\n",
        "    if prediction == 1:\n",
        "      clips.append(temp_clips)\n",
        "      temp_clips = []\n",
        "    print(temp_clips)\n",
        "    clips = [sub_clip for sub_clip in clips if sub_clip]\n",
        "    clips_by_one = clips\n",
        "    tics_count =  sum(len(sub_array) for sub_array in clips)\n",
        "    clips = [[sub_clip[0], sub_clip[-1]] for sub_clip in clips if sub_clip]\n",
        "    print(clips)\n",
        "    return clips, tics_count, clips_by_one\n",
        "  def save_video(self, video_tensor, output_path, fps=30):\n",
        "      video_tensor = video_tensor.permute(1, 2, 3, 0)\n",
        "      io.write_video(output_path, video_tensor, fps)\n",
        "  def Temporal_Detection(self, ):\n",
        "      preds = self.make_predictions()\n",
        "      preds = np.array(preds).reshape(-1)\n",
        "      clips_in_ranges, tics_count, clips = self.concatenated_clips(preds)\n",
        "      for i, (start, end) in enumerate(clips):\n",
        "          video_tensor = self.video.get_clip(start, end)\n",
        "          output_file = os.path.join(self.output_dir, f'clip_{i+1}.mp4')\n",
        "          self.save_video(video_tensor['video'], output_file)\n",
        "      return {'clips_in_ranges': clips_in_ranges, \"output_path\": self.output_dir, 'tics_count': tics_count, 'clips': clips, 'predictions':preds}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xxej6Gwr3AlW"
      },
      "outputs": [],
      "source": [
        "detect_tics = TemporalDetection('../downloaded_videos/subject1_video9.mp4',\n",
        "                                '../upload_videos')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "Osy57SEyeuK1",
        "outputId": "510e5e0a-0bda-4bdc-adab-0b6a8029b1fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video clips:  402\n",
            "(1, 2304)\n",
            "(1, 2304)\n",
            "402\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "[]\n",
            "[[2, 401]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m temporals \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_tics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTemporal_Detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn [13], line 71\u001b[0m, in \u001b[0;36mTemporalDetection.Temporal_Detection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(preds)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     70\u001b[0m clips_in_ranges, tics_count, clips \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcatenated_clips(preds)\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (start, end) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(clips):\n\u001b[0;32m     72\u001b[0m     video_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mget_clip(start, end)\n\u001b[0;32m     73\u001b[0m     output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "temporals = detect_tics.Temporal_Detection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMN2vdeXgmxk"
      },
      "outputs": [],
      "source": [
        "temporals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRqJo2shLuuj"
      },
      "outputs": [],
      "source": [
        "def concateenated_clips(labels):\n",
        "  clips = []\n",
        "  temp_clips = []\n",
        "  for count, prediction in enumerate(labels):\n",
        "    if prediction == 1:\n",
        "      temp_clips.append(count)\n",
        "    elif prediction == 0:\n",
        "      clips.append(temp_clips)\n",
        "      temp_clips = []\n",
        "  clips = [sub_clip for sub_clip in clips if sub_clip]\n",
        "  clips = [[sub_clip[0], sub_clip[-1]] for sub_clip in clips if sub_clip]\n",
        "  return clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPBUi1hkEGhT",
        "outputId": "bdc6043d-696e-4a6b-c1c6-1ef4f17c29f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video clips:  402\n",
            "(1, 2304)\n",
            "(1, 2304)\n",
            "13/13 [==============================] - 0s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "video_path = '/content/drive/MyDrive/Tics Dataset/videos/extracted videos/subject1_video9.mp4'\n",
        "preds = Temporal_Detection(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCIYXFWVLxIG"
      },
      "outputs": [],
      "source": [
        "clips = concateenated_clips(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhG8JakZW2xz"
      },
      "source": [
        "### Tell here the Temporal Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wu7ZqbEV_11"
      },
      "outputs": [],
      "source": [
        "true = pd.read_csv('/content/drive/MyDrive/Tics Dataset/SlowfastTrial/1sec_Sampling/AllCombined/1_sec_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w1B5oy8WGK3"
      },
      "outputs": [],
      "source": [
        "y_true = true[true['Video_number']==9]['labels']\n",
        "y_true = y_true.map({'normal':0, 'tic':1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdvCWVykVwon",
        "outputId": "4bd544f0-0923-4a91-9386-40064b57ee98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.17164179104477612"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(preds.reshape(-1), y_true.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT3WRW_oINGM",
        "outputId": "acc5fdab-f184-45fa-81ec-23599d633543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video clips:  402\n",
            "(1, 2304)\n"
          ]
        }
      ],
      "source": [
        "features = extract_features(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDTWoJpiIS7D",
        "outputId": "9ffbb2b5-1cf3-472f-cf75-87d1917bb2ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1, 2304, 1, 1, 1)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaOy9CoHFkj2",
        "outputId": "d65d8550-3df3-44a7-fc85-eddf944312ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7a02415555d0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import glob\n",
        "# glob.glob('/content/drive/MyDrive/Tics Dataset/SlowfastTrial/models/1 sec classification (tics temporal detection)/goodmodel.h5')\n",
        "# tf.keras.models.load_model('/content/drive/MyDrive/Tics Dataset/SlowfastTrial/models/1 sec classification (tics temporal detection)/goodmodel.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eFJ4I1bGtLi",
        "outputId": "36f444cf-d8e1-4a8d-db56-22582c124fb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2304)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# np.load('/content/drive/MyDrive/Tics Dataset/SlowfastTraial/1sec_Sampling/subject2_features/subject2_video0/patient3_video0_clip_0.npy').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxSA91A0HNZ3",
        "outputId": "bb0f384c-ad01-4326-c6da-06c80e362e7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 1, 0])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoLVUkAbHTLZ"
      },
      "outputs": [],
      "source": [
        "  def concateenated_clips( labels):\n",
        "    clips = []\n",
        "    temp_clips = []\n",
        "    for count, prediction in enumerate(labels):\n",
        "      if prediction == 1:\n",
        "        temp_clips.append(count)\n",
        "      elif prediction == 0:\n",
        "        clips.append(temp_clips)\n",
        "        temp_clips = []\n",
        "    clips = [sub_clip for sub_clip in clips if sub_clip]\n",
        "    clips_by_one = clips\n",
        "    tics_count =  sum(len(sub_array) for sub_array in clips)\n",
        "    clips = [[sub_clip[0], sub_clip[-1]] for sub_clip in clips if sub_clip]\n",
        "    return clips, tics_count, clips_by_one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPlx5vbjJHVQ",
        "outputId": "fead2a9f-ee20-4d76-a6f7-a40721a8eb02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "clips, count, values = concateenated_clips([0,1,1,1,1,1,1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5PuH_8ELJFv",
        "outputId": "7cdbd245-9bde-4bcd-e036-9a6acf837504"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([[1, 7]], 7, [[1, 2, 3, 4, 5, 6, 7]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clips, count, values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gs9asFDlwuP",
        "outputId": "c45a7f00-f425-477e-d39d-719b3b19835b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n"
          ]
        }
      ],
      "source": [
        "for i, val in enumerate(temporals['predictions'].tolist()):\n",
        "  # print(val, val == 1)\n",
        "  if val == 1:\n",
        "    print(i)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
